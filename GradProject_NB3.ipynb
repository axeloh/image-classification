{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FRn67sI_bzrm"
   },
   "source": [
    "\n",
    "<h1> DS200A Computer Vision Assignment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PsKiQRYtbzro"
   },
   "source": [
    "<h2>  Part Three: Classifier training and performance assessment. </h2>\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gc2wn_6IBun6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F2HbJKLRbzrr"
   },
   "source": [
    "#### Retrieving and preprocessing of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 424,
     "status": "error",
     "timestamp": 1574678666514,
     "user": {
      "displayName": "Henrik Høiness",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAU_UzKC8CKFaMzdXHG3v2miAUiqbdhU_utY4l9=s64",
      "userId": "05134007726078583058"
     },
     "user_tz": 480
    },
    "id": "4fkkK5APDZRb",
    "outputId": "87d21858-b8c7-461c-8982-b0b2564fb7a5"
   },
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.GradProject_NB2 import preprocess_part_one, preprocess_part_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 421,
     "status": "error",
     "timestamp": 1574675788370,
     "user": {
      "displayName": "Henrik Høiness",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAU_UzKC8CKFaMzdXHG3v2miAUiqbdhU_utY4l9=s64",
      "userId": "05134007726078583058"
     },
     "user_tz": 480
    },
    "id": "B7Qeocn8Beni",
    "outputId": "c1019e4b-4e96-4ee7-e30b-18bb86dd68a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Beginning preprocessing part one\n",
      "[INFO] Reading images\n",
      "\t- Fetching label 'airplanes'\n",
      "\t- Fetching label 'bear'\n",
      "\t- Fetching label 'blimp'\n",
      "\t\t- Gray image ('blimp_0022.jpg') was loaded, converting to RGB\n",
      "\t- Fetching label 'comet'\n",
      "\t\t- Gray image ('comet_0006.jpg') was loaded, converting to RGB\n",
      "\t\t- Gray image ('comet_0011.jpg') was loaded, converting to RGB\n",
      "\t\t- Gray image ('comet_0013.jpg') was loaded, converting to RGB\n",
      "\t\t- Gray image ('comet_0021.jpg') was loaded, converting to RGB\n",
      "\t\t- Gray image ('comet_0036.jpg') was loaded, converting to RGB\n",
      "\t\t- Gray image ('comet_0038.jpg') was loaded, converting to RGB\n",
      "\t\t- Gray image ('comet_0041.jpg') was loaded, converting to RGB\n",
      "\t\t- Gray image ('comet_0049.jpg') was loaded, converting to RGB\n",
      "\t\t- Gray image ('comet_0052.jpg') was loaded, converting to RGB\n",
      "\t\t- Gray image ('comet_0053.jpg') was loaded, converting to RGB\n",
      "\t\t- Gray image ('comet_0057.jpg') was loaded, converting to RGB\n",
      "\t\t- Gray image ('comet_0058.jpg') was loaded, converting to RGB\n",
      "\t- Fetching label 'crab'\n",
      "\t\t- Gray image ('crab_0045.jpg') was loaded, converting to RGB\n",
      "\t- Fetching label 'dog'\n",
      "\t- Fetching label 'dolphin'\n",
      "\t\t- Gray image ('dolphin_0025.jpg') was loaded, converting to RGB\n",
      "\t- Fetching label 'giraffe'\n",
      "\t- Fetching label 'goat'\n",
      "\t- Fetching label 'gorilla'\n",
      "\t\t- Gray image ('gorilla_0128.jpg') was loaded, converting to RGB\n",
      "\t- Fetching label 'kangaroo'\n",
      "\t- Fetching label 'killer-whale'\n",
      "\t- Fetching label 'leopards'\n",
      "\t- Fetching label 'llama'\n",
      "\t- Fetching label 'penguin'\n",
      "\t- Fetching label 'porcupine'\n",
      "\t- Fetching label 'teddy-bear'\n",
      "\t- Fetching label 'triceratops'\n",
      "\t- Fetching label 'unicorn'\n",
      "\t- Fetching label 'zebra'\n",
      "[INFO] Trimming and denoising images\n",
      "[INFO] Scaling images\n",
      "[INFO] Counting Hough Transform circles\n",
      "[INFO] Adding features: size, aspect_ratio, red-, green-, blue- and gray-intensity\n",
      "[INFO] Adding histogram features\n",
      "[INFO] Adding stride features\n",
      "[INFO] Done preprocessing part one.\n",
      "--------------------------------------------------\n",
      "[INFO] Beginning preprocessing part two\n",
      "[INFO] Getting descriptors\n",
      "[INFO] Building descriptor dictionary\n",
      "[INFO] Fitting KMeans with k=10 to training descriptors\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing data\n",
    "training_data = preprocess_part_one()\n",
    "train_part, val_part = train_test_split(training_data, test_size=0.1)\n",
    "\n",
    "k, desc_limit = 10, 10000\n",
    "train, val = preprocess_part_two(train_part, val_part, k, desc_limit)\n",
    "\n",
    "train_x, train_y = train.drop(columns=['Label']), train['Label']\n",
    "val_x, val_y = val.drop(columns=['Label']), val['Label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "train_x_scaled = scaler.fit_transform(train_x)\n",
    "val_x_scaled = scaler.transform(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "def rmse(actual_y, predicted_y):\n",
    "    \"\"\"\n",
    "    The root mean square error between the prediction and the ground truth\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((actual_y - predicted_y)**2)/len(predicted_y))\n",
    "\n",
    "def compute_CV_rmse_and_acc(model, X_train, Y_train):\n",
    "    '''\n",
    "    Split the training data into 5 subsets.\n",
    "    For each subset, \n",
    "        fit a model holding out that subset\n",
    "        compute the MSE on that subset (the validation set)\n",
    "    You should be fitting 5 models total.\n",
    "    Return the average MSE of these 5 folds.\n",
    "\n",
    "    Args:\n",
    "        model: an sklearn model with fit and predict functions \n",
    "        X_train (data_frame): Training data\n",
    "        Y_train (data_frame): Label \n",
    "\n",
    "    Return:\n",
    "        the average validation error and accuracy for the 5 splits.\n",
    "    '''\n",
    "    kf = KFold(n_splits=5)\n",
    "    validation_errors = []\n",
    "    validation_accuracies = []\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(X_train):\n",
    "        \n",
    "        # Split the data\n",
    "        split_X_train, split_X_valid = np.take(X_train, train_idx, axis=0), np.take(X_train, valid_idx, axis=0)\n",
    "        split_Y_train, split_Y_valid = np.take(Y_train, train_idx, axis=0), np.take(Y_train, valid_idx, axis=0)\n",
    "        \n",
    "        # Fit the model on the training split\n",
    "        model.fit(split_X_train, split_Y_train)\n",
    "        \n",
    "        # Compute the RMSE on the validation split\n",
    "        preds = model.predict(split_X_valid)\n",
    "        error = rmse(split_Y_valid, preds)\n",
    "        acc = accuracy_score(split_Y_valid, preds)\n",
    "        \n",
    "        validation_errors.append(error)\n",
    "        validation_accuracies.append(acc)\n",
    "        \n",
    "    return np.mean(validation_errors), np.mean(validation_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing 5-fold cross validation to find optimal preprocessing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# Disable\n",
    "def blockPrint():\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "# Restore\n",
    "def enablePrint():\n",
    "    sys.stdout = sys.__stdout__\n",
    "def grid_search_preprocessing_parameters():\n",
    "    for k in [10, 100, 200, 500]:\n",
    "        for down_sample in [True, False]:\n",
    "            for descriptor_limit in [6000, 10000, 30000]:\n",
    "                #blockPrint()\n",
    "                training_data = preprocess_part_one(None, True, down_sample)\n",
    "                print('-'*50)\n",
    "                train_part, val_part = train_test_split(training_data, test_size=0.1)\n",
    "                train, val = preprocess_part_two(train_part, val_part, k, descriptor_limit)\n",
    "\n",
    "                train_x, train_y = train.drop(columns=['Label']), train['Label']\n",
    "                val_x, val_y = val.drop(columns=['Label']), val['Label']\n",
    "\n",
    "                scaler = MinMaxScaler()\n",
    "                train_x_scaled = scaler.fit_transform(train_x)\n",
    "                val_x_scaled = scaler.transform(val_x)\n",
    "\n",
    "                #enablePrint()\n",
    "                print(f'**** k={k}, down_sample={down_sample}, decriptor_limit={descriptor_limit}****')\n",
    "\n",
    "                model = LogisticRegression(max_iter=500)\n",
    "                model.fit(train_x_scaled, train_y)\n",
    "                preds = model.predict(val_x_scaled)\n",
    "                print(type(model))\n",
    "                error, acc = compute_CV_rmse_and_acc(model, train_x, train_y)\n",
    "                print(f\"Cross validation mean error: {error}\")\n",
    "                print(f\"Cross validation mean accuracy: {acc}\")\n",
    "                print(f\"Test accuracy: {accuracy_score(val_y, preds)}\\n\")\n",
    "\n",
    "\n",
    "                model = KNeighborsClassifier(10, weights='distance')\n",
    "                model.fit(train_x_scaled, train_y)\n",
    "                preds = model.predict(val_x_scaled)\n",
    "                print(type(model))\n",
    "                error, acc = compute_CV_rmse_and_acc(model, train_x, train_y)\n",
    "                print(f\"Cross validation mean error: {error}\")\n",
    "                print(f\"Cross validation mean accuracy: {acc}\")\n",
    "                print(f\"Test accuracy: {accuracy_score(val_y, preds)}\\n\")\n",
    "\n",
    "                model = DecisionTreeClassifier()\n",
    "                model.fit(train_x_scaled, train_y)\n",
    "                preds = model.predict(val_x_scaled)\n",
    "                print(type(model))\n",
    "                error, acc = compute_CV_rmse_and_acc(model, train_x, train_y)\n",
    "                print(f\"Cross validation mean error: {error}\")\n",
    "                print(f\"Cross validation mean accuracy: {acc}\")\n",
    "                print(f\"Test accuracy: {accuracy_score(val_y, preds)}\\n\")\n",
    "\n",
    "                model = RandomForestClassifier(n_estimators=800)\n",
    "                model.fit(train_x_scaled, train_y)\n",
    "                preds = model.predict(val_x_scaled)\n",
    "                print(type(model))\n",
    "                error, acc = compute_CV_rmse_and_acc(model, train_x, train_y)\n",
    "                print(f\"Cross validation mean error: {error}\")\n",
    "                print(f\"Cross validation mean accuracy: {acc}\")\n",
    "                print(f\"Test accuracy: {accuracy_score(val_y, preds)}\\n\")\n",
    "\n",
    "                model = SVC(kernel='rbf',C=10, gamma=0.01)\n",
    "                model.fit(train_x_scaled, train_y)\n",
    "                preds = model.predict(val_x_scaled)\n",
    "                print(type(model))\n",
    "                error, acc = compute_CV_rmse_and_acc(model, train_x, train_y)\n",
    "                print(f\"Cross validation mean error: {error}\")\n",
    "                print(f\"Cross validation mean accuracy: {acc}\")\n",
    "                print(f\"Test accuracy: {accuracy_score(val_y, preds)}\\n\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPgsvYbhbzrs"
   },
   "source": [
    "### Logistic Regression\n",
    "##### Performing  5-fold cross validation for deciding hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1074,
     "status": "error",
     "timestamp": 1574642755220,
     "user": {
      "displayName": "Axel Oevreboe Harstad",
      "photoUrl": "",
      "userId": "00313732855330319553"
     },
     "user_tz": 480
    },
    "id": "p_F4f5Slbzrt",
    "outputId": "4d14a70d-77fb-43e6-98f0-24c5cb84814c"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(multi_class='multinomial', solver= 'lbfgs', penalty='l2', max_iter=1000)\n",
    "\n",
    "error, acc = compute_CV_rmse_and_acc(model, train_x_scaled, train_y)\n",
    "print(f\"Mean RMSE: {error}\")\n",
    "print(f\"Accuracy: {acc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NcCxllL3bzrv"
   },
   "source": [
    "### K-nearest Neighbors\n",
    "##### Predicting training set with 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OLQdVbRAbzrw"
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=10, weights='distance')\n",
    "\n",
    "error, acc = compute_CV_rmse_and_acc(model, train_x_scaled, train_y)\n",
    "print(f\"Mean RMSE: {error}\")\n",
    "print(f\"Accuracy: {acc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eyf9J_TLbzry"
   },
   "source": [
    "### Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LPQlFzuebzry"
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "error, acc = compute_CV_rmse_and_acc(model, train_x_scaled, train_y)\n",
    "print(f\"Mean RMSE: {error}\")\n",
    "print(f\"Accuracy: {acc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_T-JIq8bzr1"
   },
   "source": [
    "### Random Forest\n",
    "##### Predicting training set with 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZ8TnM93bzr1"
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=800, random_state=42)\n",
    "error, acc = compute_CV_rmse_and_acc(model, train_x_scaled, train_y)\n",
    "print(f\"Mean RMSE: {error}\")\n",
    "print(f\"Accuracy: {acc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing importances\n",
    "s = model.feature_importances_\n",
    "index_importance_sorted = sorted(range(len(s)), key=lambda k: s[k], reverse=True)\n",
    "top_index = index_importance_sorted[:50]\n",
    "\n",
    "print(\"\\nMost importance features:\")\n",
    "for index in top_index:\n",
    "    print(f\"Feature name: {train_x.columns[index]}, Importance={s[index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing K-fold grid search to find optimal parameters for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_params())\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "\n",
    "#rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "#rf_random.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    preds = model.predict(test_features)\n",
    "    accuracy = accuracy_score(preds, test_labels)\n",
    "    print('Accuracy = {:0.2f}%.'.format(100*accuracy))\n",
    "    return accuracy\n",
    "\n",
    "base_model = RandomForestClassifier(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(train_x, train_y)\n",
    "base_accuracy = evaluate(base_model, val_x, val_y)\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, val_x, val_y)\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iTudbrXEbzr4"
   },
   "outputs": [],
   "source": [
    "model = SVC(kernel='rbf',C=10, gamma=0.01, decision_function_shape='ovo')\n",
    "\n",
    "error, acc = compute_CV_rmse_and_acc(model, train_x_scaled, train_y)\n",
    "print(f\"Mean RMSE: {error}\")\n",
    "print(f\"Accuracy: {acc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Logistic_regression\n",
    "* https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest\n",
    "* https://scikit-learn.org/stable/modules/tree.html#tree\n",
    "* https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "* https://en.wikipedia.org/wiki/Random_forest\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "* https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GradProject_NB3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
